ğŸ¡ End-to-End Machine Learning Project â€” California Housing Price Prediction

This project is a complete end-to-end machine learning workflow, built from scratch following industry best practices.
It demonstrates my ability to:

Build reproducible data pipelines

Perform exploratory data analysis (EDA)

Engineer meaningful features

Train, evaluate, and fine-tune ML models

Use Scikit-Learn pipelines and custom transformers

Apply hyperparameter tuning (GridSearchCV)

Handle model persistence and deployment considerations

Understand real-world ML operations (monitoring, versioning, drift handling)

This is a foundational ML project that shows not only that I understand the theory, but that I can implement practical, production-oriented ML systems.

ğŸ“ Project Structure
ğŸ“¦ End-to-End-ML-Project
â”œâ”€â”€ data/                    # Raw and processed dataset
â”œâ”€â”€ notebook.ipynb           # Full development notebook
â”œâ”€â”€ models/                  # Saved model artifacts (joblib)
â”œâ”€â”€ README.md                # Project documentation
â””â”€â”€ images/                  # Visualizations for README

ğŸ¯ Objective

Predict median housing prices in California districts using the classic California Housing dataset.

This project walks through the entire lifecycle of an ML systemâ€”from raw data to a production-ready model.

ğŸ” Key Features & What I Implemented
1ï¸âƒ£ Data Ingestion & Test Set Creation

Automated dataset download via Python.

Created a stratified train/test split based on income categories to ensure representative sampling.

Ensured reproducibility using Scikit-Learn utilities.

2ï¸âƒ£ Exploratory Data Analysis (EDA)

Detected correlations using scatter plots and correlation matrices.

Visualized geospatial patterns in median house values (latitude & longitude).

Identified important predictors such as:

Median Income

Proximity to coast

Rooms per household

3ï¸âƒ£ Data Cleaning & Feature Engineering

Implemented a full preprocessing pipeline:

âœ” Handling missing values via imputation
âœ” Custom feature creation (e.g., rooms_per_household, bedrooms_ratio)
âœ” Scaling numerical features
âœ” One-hot encoding for categorical attributes
âœ” Custom ClusterSimilarity Transformer to compute cluster-based RBF similarity features
âœ” Integrated everything using a Scikit-Learn ColumnTransformer & Pipeline

4ï¸âƒ£ Model Training

Trained multiple models to compare baseline performance:

Linear Regression

Decision Tree

Random Forest

XGBoost (validates modern ML methods)

Evaluated using RMSE and validated through cross-validation for reliable performance estimates.

5ï¸âƒ£ Hyperparameter Tuning (GridSearchCV)

Used GridSearchCV to optimize:

RandomForest hyperparameters (max_features, n_estimators)

ClusterSimilarity hyperparameters inside the preprocessing pipeline

Search executed through 3-fold CV with custom scoring (neg_root_mean_squared_error)

Demonstrates understanding of:

Pipeline hyperparameter naming (preprocessing__geo__n_clusters)

Efficient experiment management

Avoiding data leakage through encapsulated pipelines

6ï¸âƒ£ Final Model & Evaluation

After fine-tuning:

Selected best model

Evaluated on held-out test set

Examined error distribution

Compared predictions vs actual values

7ï¸âƒ£ Model Persistence (Production Preparation)

Saved final model using joblib:

joblib.dump(final_model, "my_california_housing_model.pkl")


Documented how to:

Reload model in production

Handle custom transformers on reload

Build REST API wrapper (FastAPI or Flask)

Deploy the model to cloud platforms (Vertex AI, AWS Sagemaker)

ğŸ§  What This Project Demonstrates About My Skills

This project shows that I can:

âœ” Build real, scalable ML systems â€” not just notebook experiments
âœ” Use Scikit-Learn at an advanced level (pipelines, transformers, hyperparameter grids)
âœ” Apply solid software engineering practices
âœ” Understand MLOps fundamentals such as:

model versioning

monitoring model drift

test-set preservation

automated retraining

âœ” Communicate ML results clearly and professionally
ğŸ›  Technologies Used

Python

NumPy / Pandas

Matplotlib / Seaborn

Scikit-Learn

Joblib

XGBoost

Jupyter Notebook

ğŸš€ Future Improvements

Deploy as an interactive Streamlit web app

Build a FastAPI prediction API

Add monitoring dashboards (data drift & performance decay)

Experiment with deep learning approaches for non-linear patterns

ğŸ“œ License

MIT License
